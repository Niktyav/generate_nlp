# Домашнее задание 2. Разработка чат-бота на основе генеративной модели.

Необходимо разработать чат-бот, используя генеративный подход. Бот должен вести диалог как определенный персонаж сериала, имитируя стиль и манеру конкретного персонажа сериала. Важно учесть особенности речи и темы, которые поднимает персонаж, его типичные реакции.


# Описание проекта

Данный проект представляет генеративного чат-бота, имитирующего манеру речи Чендлера Бинга – одного из главных персонажей ситкома «Друзья». Цель проекта – создать диалоговую модель, способную отвечать пользователю с фирменным сарказмом и юмором Чендлера. Это решает задачу стилизации ответов под конкретного персонажа, что может использоваться для развлекательных чат-ботов или творческих приложений. В основе бота лежит предобученная языковая модель (Llama 1B), дообученная на репликах Чендлера из сценария сериала. Модель была дообучена методами fine-tuning на специальном датасете диалогов, чтобы научиться генерировать ответы в стиле Чендлера.

# Данные

Основой для обучения послужили два источника данных:  
    -- Сценарии сериала «Друзья» – датасет с репликами персонажей сериала за 10 сезонов (Kaggle: Friends TV Scripts)​
    https://www.kaggle.com/datasets/amandam1/friends-scripts  Из него извлекались диалоговые пары вопрос–ответ, где ответом была реплика Чендлера Бинга, а вопросом – предыдущая реплика другого персонажа (контекст разговора). Каждая такая пара представляет мини-диалог: на фразу собеседника модель должна выдать ответ в стиле Чендлера.   
    -- Сценарии фильмов (IMSDB) – подборка скриптов различных фильмов с IMSDB​ https://imsdb.com/all-scripts.html Они использовались для формирования нерелевантных примеров и негативных классов при обучении вспомогательной модели-классификатора. Эти данные помогают убедиться, что бот отличает “чужой” стиль от речи Чендлера.
    
Предобработка данных:  Для сценариев «Друзей» данные преобразованы в структуру с колонками: character (имя говорящего персонажа), q (реплика-вопрос/контекст) и a (реплика-ответ). Отфильтрованы диалоги так, чтобы ответ a принадлежал Чендлеру. В итоге получилось около 8 189 пар реплик, из которых ~90% пошло на обучение (7370 примеров) и 10% на тестирование (819 примеров).  Для датасета IMSDB были случайным образом выбраны реплики, не относящиеся к Чендлеру, для обучения классификатора стиля.


# Обучение модели

Модель для чат-бота была обучена методом тонкой настройки (fine-tuning) на собранных репликах Чендлера. В качестве исходной использована компактная языковая модель Llama 3.2 – 1B Instruct от проекта Unsloth (около 1 млрд параметров, оригинальная LLAMA 3.2 от запрещенной организации недоступна). Чтобы эффективно дообучить такую большую модель на доступной GPU, применялись техники сжатия и адаптации:

    *Квантование 4-bit (QLoRA): базовая модель загружалась в 4-битном формате с помощью bitsandbytes для снижения требуемой памяти.
    Parameter-Efficient Fine-Tuning: использовалась библиотека PEFT (метод LoRA) для обучения только небольшого числа дополнительных параметров. Конфигурация LoRA: размер внутреннего представления r=16, коэффициент α=32, dropout=0.05 – такие параметры позволяют обучить адаптацию, не изменяя исходные весы модели.
    *Гиперпараметры обучения: одна эпоха по всему обучающему набору (эпох больше не использовали во избежание переобучения). Размер батча – 1 (эффективный батч 2 за счет gradient_accumulation_steps=2), алгоритм оптимизации AdamW (режим 32-bit оптимизатора для 4-bit весов), шаг обучения learning rate = 2e-4, стратегия warmup_steps=10. Обучение проводилось в смешанной точности (BF16) на GPU. Логирование велось каждые 50–100 шагов, оценка на валидационной выборке – каждые 50 шагов.

В ходе обучения наблюдалось устойчивое снижение функции потерь как на тренировочных, так и на тестовых данных. На графике ниже показана динамика training loss и validation loss в процессе обучения модели:

Рис. 1: График обучения модели – изменение функции потерь на обучающей (оранжевая линия) и проверочной (красная линия) выборках по мере тренировки.

После одной эпохи финальное значение ошибки на валидации составило ~2.72, снизившись с ~3.17 в начале обучения. Обучение заняло около ~40 минут на GPU (Mobile RTX 4090). Дообученная модель сохранена под именем “llama-3.1-8b-chat-chandler” (а также экспортирована в формат LLaMA.CPP для запуска вне Python).
# Структура репозитория

hw1/   
├── data/    
│ ├── other/ # сценарии фильмов    
│ └── friends.csv  # датасет реплик друзей   
├── models/ # Сохранённые модели ( llama-3.1-8b-chat-chandler и cдоступны по [ссылке](https://disk.yandex.ru/d/WigCN0l2PrsCGg)   
├── chatbot/   
│ ├── requirements.txt # Зависимости проекта   
│ ├── dockerfile # Конфигурация контейнера Flask  
│ └── app.py # Flask приложение для веб-сервиса   
├── img/ # Графики и иллюстрации   
│ ├── gradio_demo.jpg # Пример диалога  
│ ├── chandler_flask_demo.jpg # пример диалога   
│ ├── biencoder_learning.jpg # График сходимости Bi-Encoder   
│ └── crossencoder_learning.jpg# График сходимости Cross-Encoder    
├── docker-compose.yml # Файл для контейнеризации (Flask, Llama.cpp и т.д.)    
├── friends_generate.ipynb # Ноутбук с кодом предобработки данных, обучением моделей и инференсом в gradio   
└── README.md # Отчёт (данный файл)   

