{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frends\n",
    "\n",
    "    «Друзья» (англ. Friends) — американский ситком, повествующий о жизни шестерых друзей. Признан одним из лучших комедийных сериалов в истории американского телевидения и стал одним из наиболее знаменитых проектов 1990-х годов[2]. Сама постановка и творческая группа получили множество наград, в том числе шесть премий «Эмми» и премию «Золотой глобус»[3].\n",
    "\n",
    "Че́ндлер Бинг (Мэттью Перри) — полное имя Че́ндлер Мюриэл Бинг\n",
    "\n",
    "Чендлер родился в семье писательницы эротических романов Норы Тайлер Бинг и гомосексуала-трансвестита Чарльза Бинга. У Чендлера множество комплексов, от которых он отгораживается постоянными шутками. Работа Чендлера — анализ и реконфигурация данных, и в этой сфере он делает неплохую карьеру. Однако в конце сериала он бросает работу и начинает всё с нуля в рекламном бизнесе. Чендлеру не очень везёт в любви. По существу, у него было всего два более-менее серьёзных романа: с Дженис и с Моникой, на которой он женится в седьмом сезоне сериала.\n",
    "\n",
    "\n",
    " Диалоги формаировались на основании https://www.kaggle.com/datasets/amandam1/friends-scripts\n",
    " Диалог - это вопрос(контекст - предыдущая реплика)  - ответ персонажа\n",
    "\n",
    " Поиск ведется по наиболее релевантному контексту и отдается реплика персонажа\n",
    "\n",
    "\n",
    "Нерелевантные примеры  формировались на соновании сценариев к фильмам взятым с   https://imsdb.com/all-scripts.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T19:19:36.421057Z",
     "iopub.status.busy": "2024-07-31T19:19:36.420121Z",
     "iopub.status.idle": "2024-07-31T19:20:22.145747Z",
     "shell.execute_reply": "2024-07-31T19:20:22.144324Z",
     "shell.execute_reply.started": "2024-07-31T19:19:36.421009Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "\n",
    "# %pip install -U peft\n",
    "# %pip install -U trl\n",
    "# %pip install -U bitsandbytes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T19:20:22.148355Z",
     "iopub.status.busy": "2024-07-31T19:20:22.148024Z",
     "iopub.status.idle": "2024-07-31T19:20:44.164336Z",
     "shell.execute_reply": "2024-07-31T19:20:44.163538Z",
     "shell.execute_reply.started": "2024-07-31T19:20:22.148324Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, torch, mlflow\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, setup_chat_format, DataCollatorForCompletionOnlyLM\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# зафиксируем случайности\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T19:20:46.969022Z",
     "iopub.status.busy": "2024-07-31T19:20:46.967876Z",
     "iopub.status.idle": "2024-07-31T19:20:46.974308Z",
     "shell.execute_reply": "2024-07-31T19:20:46.973252Z",
     "shell.execute_reply.started": "2024-07-31T19:20:46.968992Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct\"  # со страницы запрещенной организации недоступна\n",
    "    new_model = \"llama-3.1-8b-chat-chandler\"\n",
    "    torch_dtype = torch.float16\n",
    "    attn_implementation = \"eager\"\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# настройки  \n",
    "\n",
    "# Персонаж для диалога\n",
    "persona = 'chandler'\n",
    "\n",
    "output= f\"{persona}.json\"\n",
    "other_output='other.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных\n",
    "\n",
    "берем данные и код обработки из первого ДЗ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# фунция обработки датасета Друзей\n",
    "\n",
    "def extract_dialogs(filename, character_name=None):\n",
    "    # Загрузка данных\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    # Группировка и очистка текста\n",
    "    grouped = []\n",
    "    prev_char, current_text = None, []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Очистка от \\xa0 и лишних пробелов\n",
    "        cleaned_line = row['line'].replace('\\xa0', ' ').strip()\n",
    "        if row['character'] == prev_char:\n",
    "            current_text.append(cleaned_line)\n",
    "        else:\n",
    "            if prev_char is not None:\n",
    "                grouped_text = ' '.join(current_text)\n",
    "                grouped.append({'character': prev_char, 'text': grouped_text})\n",
    "            prev_char = row['character']\n",
    "            current_text = [cleaned_line]\n",
    "    # Добавляем последнюю группу\n",
    "    if prev_char is not None:\n",
    "        grouped_text = ' '.join(current_text)\n",
    "        grouped.append({'character': prev_char, 'text': grouped_text})\n",
    "    \n",
    "    # Формирование пар\n",
    "    dialogs = []\n",
    "    for i in range(len(grouped) - 1):\n",
    "        question = grouped[i]['text']\n",
    "        answer = grouped[i + 1]\n",
    "        dialogs.append({\n",
    "            'character': answer['character'],  # Ответивший\n",
    "            'q': question,\n",
    "            'a': answer['text']\n",
    "        })\n",
    "    \n",
    "    # Фильтрация по отвечающему\n",
    "    if character_name:\n",
    "        dialogs = [d for d in dialogs if d['character'] == character_name]\n",
    "    \n",
    "    return dialogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'character': 'chandler',\n",
       "  'q': \"c'mon, you're going out with the guy! there's gotta be something wrong with him!\",\n",
       "  'a': 'all right joey, be nice.  so does he have a hump? a hump and a hairpiece?'},\n",
       " {'character': 'chandler',\n",
       "  'q': \"okay, everybody relax. this is not even a date. it's just two people going out to dinner and- not having sex.\",\n",
       "  'a': \"sounds like a date to me. alright, so i'm back in high school, i'm standing in the middle of the cafeteria, and i realize i am totally naked.\"},\n",
       " {'character': 'chandler',\n",
       "  'q': 'oh, yeah. had that dream.',\n",
       "  'a': \"then i look down, and i realize there's a phone... there.\"}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# получим все ответы персонажа с контекстом вопросом\n",
    "person_dialogs = extract_dialogs('./data/friends.csv', persona)\n",
    "\n",
    "# сохраним диалоги в файл\n",
    "with open(output, 'w', encoding='utf-8') as file:\n",
    "    json.dump(person_dialogs, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Первые 3 ответа персонажа\n",
    "person_dialogs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция обработки текстового формата сценариев и извлечения диалогов\n",
    "def parse_dialogs(file_path, target_character=None):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        script_text = f.read()    \n",
    "    dialogs = []\n",
    "    current_dialog = None\n",
    "    previous_dialog = None\n",
    "    actions_between = []\n",
    "    \n",
    "    character_pattern = re.compile(r'^([A-Z][A-Z\\s\\-\\']+)(\\s+\\(.*?\\))?\\s*$')\n",
    "    \n",
    "    for line in script_text.split('\\n'):\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Определяем строку с персонажем\n",
    "        match = character_pattern.match(line)\n",
    "        if match:\n",
    "            # Сохраняем предыдущий диалог если он соответствует фильтру\n",
    "            if current_dialog:\n",
    "                if target_character is None or current_dialog['character'] == target_character:\n",
    "                    dialogs.append({'character':current_dialog['character'],'q': ' '.join(current_dialog['context']), 'a' : ' '.join(current_dialog['lines'])})\n",
    "                previous_dialog = current_dialog\n",
    "                \n",
    "            # Создаем новый диалог\n",
    "            current_character = match.group(1).strip()\n",
    "            current_dialog = {\n",
    "                'character': current_character,\n",
    "                'lines': [],\n",
    "                'context': []\n",
    "            }\n",
    "            \n",
    "            # Добавляем контекст: предыдущая реплика + действия\n",
    "            if previous_dialog:\n",
    "                prev_lines = f\"{previous_dialog['character']}: {' '.join(previous_dialog['lines'])}\"\n",
    "                current_dialog['context'].append(prev_lines)\n",
    "            current_dialog['context'].extend(actions_between)\n",
    "            actions_between = []\n",
    "            continue\n",
    "        \n",
    "        # Собираем действия между репликами\n",
    "        if line.startswith('(') and line.endswith(')'):\n",
    "            action = line[1:-1].strip()\n",
    "            if current_dialog:\n",
    "                current_dialog['context'].append(action)\n",
    "            else:\n",
    "                actions_between.append(action)\n",
    "            continue\n",
    "        \n",
    "        # Собираем текст реплики\n",
    "        if current_dialog and line:\n",
    "            current_dialog['lines'].append(line)\n",
    "    \n",
    "    # Добавляем последний диалог если соответствует фильтру\n",
    "    if current_dialog and (target_character is None or current_dialog['character'] == target_character):\n",
    "        dialogs.append({'character':current_dialog['character'],'q': ' '.join(current_dialog['context']), 'a' : ' '.join(current_dialog['lines'])})\n",
    "    \n",
    "    return dialogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция обработки файлов папки\n",
    "def process_all_files(persona=\"THREEPIO\", data_dir='data', output_file='all_answers.json'):\n",
    "    all_dialogues = []\n",
    "    \n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                input_file = os.path.join(root, file)                 \n",
    "                print(f\"Обработка файла: {input_file}\")                \n",
    "                dialogs = parse_dialogs(input_file, persona)\n",
    "                all_dialogues.extend(dialogs)\n",
    "\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(all_dialogues, file, ensure_ascii=False, indent=4)\n",
    "        print(f\"JSON-файл сохранен: {output_file}, записано строк: {len(all_dialogues)}\")                \n",
    "    print(f\"Объединенный JSON-файл сохранен как: {output_file}\")\n",
    "    return all_dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработка файла: data/other\\KUNG FU PANDA.txt\n",
      "Обработка файла: data/other\\PEARL HARBOR.txt\n",
      "Обработка файла: data/other\\SW_Episode1.txt\n",
      "Обработка файла: data/other\\SW_Episode2.txt\n",
      "Обработка файла: data/other\\SW_Episode3.txt\n",
      "Обработка файла: data/other\\SW_Episode4.txt\n",
      "Обработка файла: data/other\\SW_Episode5.txt\n",
      "Обработка файла: data/other\\SW_Episode6.txt\n",
      "Обработка файла: data/other\\SW_Episode7.txt\n",
      "Обработка файла: data/other\\WILD WILD WEST.txt\n",
      "JSON-файл сохранен: other.json, записано строк: 8474\n",
      "Объединенный JSON-файл сохранен как: other.json\n"
     ]
    }
   ],
   "source": [
    "# Запуск обработки файлов сценариев из других вселенных для создания \"неправильных\" ответов\n",
    "other_dialogs = process_all_files(persona = None, data_dir='data/other', output_file=other_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8474,\n",
       " {'character': 'SHIFU',\n",
       "  'q': 'OOGWAY: One often meets his destiny on the road he takes to avoid it.',\n",
       "  'a': \"We have to do something. We can't just let him march on the valley, and take his revenge! He'll, he'll-- Oogway looks into the water of the moon pool.\"})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(other_dialogs), other_dialogs[77]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка к обучению"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T19:20:46.975766Z",
     "iopub.status.busy": "2024-07-31T19:20:46.975431Z",
     "iopub.status.idle": "2024-07-31T19:23:16.014332Z",
     "shell.execute_reply": "2024-07-31T19:23:16.013378Z",
     "shell.execute_reply.started": "2024-07-31T19:20:46.975741Z"
    }
   },
   "outputs": [],
   "source": [
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=cfg.torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    cfg.model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=cfg.attn_implementation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T19:23:16.015907Z",
     "iopub.status.busy": "2024-07-31T19:23:16.015614Z",
     "iopub.status.idle": "2024-07-31T19:23:18.405382Z",
     "shell.execute_reply": "2024-07-31T19:23:18.404549Z",
     "shell.execute_reply.started": "2024-07-31T19:23:16.015883Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
    "if hasattr(tokenizer, \"chat_template\") and tokenizer.chat_template is not None:\n",
    "    tokenizer.chat_template = None  # Reset the chat template\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.padding_token = '<|pad|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T19:23:18.406871Z",
     "iopub.status.busy": "2024-07-31T19:23:18.406587Z",
     "iopub.status.idle": "2024-07-31T19:23:19.566904Z",
     "shell.execute_reply": "2024-07-31T19:23:19.565881Z",
     "shell.execute_reply.started": "2024-07-31T19:23:18.406847Z"
    }
   },
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = load_dataset(\"json\", data_files=output)\n",
    "dataset = dataset_dict[\"train\"] if \"train\" in dataset_dict else dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T19:23:24.467754Z",
     "iopub.status.busy": "2024-07-31T19:23:24.467426Z",
     "iopub.status.idle": "2024-07-31T19:23:24.473839Z",
     "shell.execute_reply": "2024-07-31T19:23:24.472918Z",
     "shell.execute_reply.started": "2024-07-31T19:23:24.467728Z"
    }
   },
   "outputs": [],
   "source": [
    "#приведение к формату чата\n",
    "def format_chat_template(row):\n",
    "    row_json = [{\"role\": \"user\", \"content\": row[\"q\"]},\n",
    "               {\"role\": \"assistant\", \"content\": row[\"a\"]}]\n",
    "    text = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем data collator, который определяет, откуда считать лосс\n",
    "response_template = \"assistant\\n\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T19:23:39.359682Z",
     "iopub.status.busy": "2024-07-31T19:23:39.359332Z",
     "iopub.status.idle": "2024-07-31T19:23:41.449434Z",
     "shell.execute_reply": "2024-07-31T19:23:41.448661Z",
     "shell.execute_reply.started": "2024-07-31T19:23:39.35965Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['character', 'q', 'a'],\n",
       "        num_rows: 7370\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['character', 'q', 'a'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sh = dataset.shuffle(seed=2024).select(range(len(dataset)))\n",
    "dataset_sh = dataset_sh.train_test_split(0.1)\n",
    "dataset_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T19:40:20.141299Z",
     "iopub.status.busy": "2024-07-31T19:40:20.1409Z",
     "iopub.status.idle": "2024-07-31T19:40:20.182873Z",
     "shell.execute_reply": "2024-07-31T19:40:20.182009Z",
     "shell.execute_reply.started": "2024-07-31T19:40:20.141268Z"
    }
   },
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=cfg.new_model,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1,#ставим одну эпоху, со второй идет переобучение\n",
    "    #max_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    logging_steps=100,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    group_by_length=True,\n",
    "    logging_dir='./logs',\n",
    "    report_to=[\"mlflow\"],  # Логирование только в MLflow        \n",
    "    run_name=\"Llama-3.2-chandler\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T19:40:21.439957Z",
     "iopub.status.busy": "2024-07-31T19:40:21.439288Z",
     "iopub.status.idle": "2024-07-31T19:40:27.98408Z",
     "shell.execute_reply": "2024-07-31T19:40:27.983305Z",
     "shell.execute_reply.started": "2024-07-31T19:40:21.439924Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_sh[\"train\"],\n",
    "    eval_dataset=dataset_sh[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    processing_class = tokenizer,\n",
    "    formatting_func=format_chat_template,\n",
    "    args=training_arguments,\n",
    "    data_collator=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T19:40:31.097638Z",
     "iopub.status.busy": "2024-07-31T19:40:31.097229Z",
     "iopub.status.idle": "2024-07-31T20:09:00.849422Z",
     "shell.execute_reply": "2024-07-31T20:09:00.847937Z",
     "shell.execute_reply.started": "2024-07-31T19:40:31.097607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3685' max='3685' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3685/3685 38:40, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.166881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.133500</td>\n",
       "      <td>3.100621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.133500</td>\n",
       "      <td>3.093056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.901700</td>\n",
       "      <td>3.071405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.901700</td>\n",
       "      <td>3.089561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.975000</td>\n",
       "      <td>2.990439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.975000</td>\n",
       "      <td>2.981388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.988000</td>\n",
       "      <td>3.022996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.988000</td>\n",
       "      <td>2.978867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.869100</td>\n",
       "      <td>2.982294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.869100</td>\n",
       "      <td>2.951311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.895200</td>\n",
       "      <td>2.988369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.895200</td>\n",
       "      <td>2.949891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.018600</td>\n",
       "      <td>2.928041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>3.018600</td>\n",
       "      <td>2.915231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.818300</td>\n",
       "      <td>2.925226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>2.818300</td>\n",
       "      <td>2.940824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.844200</td>\n",
       "      <td>2.906129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>2.844200</td>\n",
       "      <td>2.909104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.832900</td>\n",
       "      <td>2.898242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>2.832900</td>\n",
       "      <td>2.937169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.901300</td>\n",
       "      <td>2.874206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>2.901300</td>\n",
       "      <td>2.858448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.816100</td>\n",
       "      <td>2.886544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>2.816100</td>\n",
       "      <td>2.865190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.767000</td>\n",
       "      <td>2.906140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>2.767000</td>\n",
       "      <td>2.872535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.851800</td>\n",
       "      <td>2.886819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>2.851800</td>\n",
       "      <td>2.879956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.772900</td>\n",
       "      <td>2.855277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>2.772900</td>\n",
       "      <td>2.837271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.886200</td>\n",
       "      <td>2.833528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>2.886200</td>\n",
       "      <td>2.828688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.772000</td>\n",
       "      <td>2.827073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>2.772000</td>\n",
       "      <td>2.840058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.855100</td>\n",
       "      <td>2.827391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>2.855100</td>\n",
       "      <td>2.807405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.896000</td>\n",
       "      <td>2.801678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>2.896000</td>\n",
       "      <td>2.830462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.733900</td>\n",
       "      <td>2.791549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>2.733900</td>\n",
       "      <td>2.791334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.794400</td>\n",
       "      <td>2.794505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>2.794400</td>\n",
       "      <td>2.795688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.815000</td>\n",
       "      <td>2.776564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>2.815000</td>\n",
       "      <td>2.784329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.831400</td>\n",
       "      <td>2.786372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>2.831400</td>\n",
       "      <td>2.779603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.747800</td>\n",
       "      <td>2.774746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>2.747800</td>\n",
       "      <td>2.765277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.764500</td>\n",
       "      <td>2.777533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>2.764500</td>\n",
       "      <td>2.764077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.813100</td>\n",
       "      <td>2.753483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>2.813100</td>\n",
       "      <td>2.743445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.879400</td>\n",
       "      <td>2.748454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>2.879400</td>\n",
       "      <td>2.742519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.872600</td>\n",
       "      <td>2.736953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>2.872600</td>\n",
       "      <td>2.748865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.757400</td>\n",
       "      <td>2.745991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>2.757400</td>\n",
       "      <td>2.744645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.772300</td>\n",
       "      <td>2.748719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>2.772300</td>\n",
       "      <td>2.747244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>2.792900</td>\n",
       "      <td>2.734431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>2.792900</td>\n",
       "      <td>2.740060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.700200</td>\n",
       "      <td>2.741097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>2.700200</td>\n",
       "      <td>2.732773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>2.853000</td>\n",
       "      <td>2.729242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>2.853000</td>\n",
       "      <td>2.724328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.731900</td>\n",
       "      <td>2.720119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>2.731900</td>\n",
       "      <td>2.723237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.733900</td>\n",
       "      <td>2.727035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>2.733900</td>\n",
       "      <td>2.722879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.796400</td>\n",
       "      <td>2.723596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>2.796400</td>\n",
       "      <td>2.720189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\rvv19\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\utils\\save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\rvv19\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\utils\\save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\rvv19\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\utils\\save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\rvv19\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\utils\\save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\rvv19\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\utils\\save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\rvv19\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\utils\\save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\rvv19\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\utils\\save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\rvv19\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\utils\\save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3685, training_loss=2.8355348253185424, metrics={'train_runtime': 2320.1983, 'train_samples_per_second': 3.176, 'train_steps_per_second': 1.588, 'total_flos': 1979661271670784.0, 'train_loss': 2.8355348253185424})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T20:09:40.932639Z",
     "iopub.status.busy": "2024-07-31T20:09:40.931936Z",
     "iopub.status.idle": "2024-07-31T20:10:11.058559Z",
     "shell.execute_reply": "2024-07-31T20:10:11.057552Z",
     "shell.execute_reply.started": "2024-07-31T20:09:40.932606Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rvv19\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\utils\\save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Llama-finetuned\\\\tokenizer_config.json',\n",
       " 'Llama-finetuned\\\\special_tokens_map.json',\n",
       " 'Llama-finetuned\\\\tokenizer.json')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_save = \"Llama-finetuned\"\n",
    "trainer.save_model(path_to_save)\n",
    "model.save_pretrained(path_to_save)\n",
    "tokenizer.save_pretrained(path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Определение качества соответствия генерации   \n",
    "\n",
    "Подготовим данные и обучим классификатор, который определяет является ли текст репликой Chandler, это позволит оценить качество обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Dad Dad Dad, it was just a dream.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5804</th>\n",
       "      <td>He will join us or die, my master. Vader kneel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7530</th>\n",
       "      <td>Precinct 47. Here.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>Just feeling this good was worth it. In the ba...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16254</th>\n",
       "      <td>uhm, we've got turkey grease.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7728</th>\n",
       "      <td>I could sure use those clothes now. She respon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8251</th>\n",
       "      <td>Yeah, it would. That's Loveless' Lair. 140   E...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3233</th>\n",
       "      <td>You have fought gallantly. Worthy of recogniti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>Your Highness, we are the Ambassadors, for the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2518</th>\n",
       "      <td>Yes, Master…</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "47                     Dad Dad Dad, it was just a dream.      0\n",
       "5804   He will join us or die, my master. Vader kneel...      0\n",
       "7530                                  Precinct 47. Here.      0\n",
       "2099   Just feeling this good was worth it. In the ba...      0\n",
       "16254                      uhm, we've got turkey grease.      1\n",
       "7728   I could sure use those clothes now. She respon...      0\n",
       "8251   Yeah, it would. That's Loveless' Lair. 140   E...      0\n",
       "3233   You have fought gallantly. Worthy of recogniti...      0\n",
       "1730   Your Highness, we are the Ambassadors, for the...      0\n",
       "2518                                        Yes, Master…      0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузка данных из JSON файла\n",
    "df_json_ch = pd.read_json(output)\n",
    "# Загрузка данных из CSV файла\n",
    "df_json_other = pd.read_json(other_output)\n",
    "\n",
    "\n",
    "# Объединение датафреймов\n",
    "df = pd.concat([df_json_other, df_json_ch], ignore_index=True)\n",
    "\n",
    "# Создание колонки 'chandler_answer': 1, если персонаж — \"chandler\", иначе 0\n",
    "df['label'] = df['character'].apply(lambda x: 1 if str(x).lower() == 'chandler' else 0)\n",
    "\n",
    "# Формирование итогового датафрейма с колонками \"text\" (из поля \"a\") и \"chandler_answer\"\n",
    "result_df = df[['a', 'label']].rename(columns={'a': 'text'})\n",
    "\n",
    "# Вывод первых строк итогового датафрейма\n",
    "result_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93aa28420b7d465d909090e145eb4297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13330 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee343bfa44104a0abbce0c281072cee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3333 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\rvv19\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1667' max='1667' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1667/1667 01:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.081300</td>\n",
       "      <td>0.246002</td>\n",
       "      <td>0.912991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='417' max='417' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [417/417 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.2460024058818817, 'eval_accuracy': 0.912991299129913, 'eval_runtime': 6.1188, 'eval_samples_per_second': 544.718, 'eval_steps_per_second': 68.151, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Предполагается, что у вас уже есть датафрейм result_df с колонками 'text' и 'chandler_answer'\n",
    "df = result_df.dropna(subset=['text', 'label'])\n",
    "\n",
    "# Разделение данных\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Преобразование датафреймов в Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Загрузка токенизатора\n",
    "tokenizer_classification = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Функция токенизации\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer_classification(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Токенизация датасетов\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Установка формата для PyTorch\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Загрузка модели BERT для классификации (2 класса)\n",
    "model_classific = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Параметры обучения\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=[\"mlflow\"],  # Логирование только в MLflow     \n",
    "    run_name=\"chandler_classifier\",\n",
    ")\n",
    "\n",
    "# Загрузка метрики через evaluate\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Функция для вычисления метрик\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Создание Trainer\n",
    "trainer = Trainer(\n",
    "    model=model_classific,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Обучение модели\n",
    "trainer.train()\n",
    "\n",
    "# Оценка модели\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", eval_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сравнение работы обученной и не обученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выделение ответов\n",
    "def generate_answer(model, prompt):\n",
    "    chat = [\n",
    "        { \"role\": \"user:\", \"content\": prompt },\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=25, do_sample=True, temperature=0.5)\n",
    "    conversation = tokenizer.decode(outputs[0])\n",
    "    match = re.search(r\"<\\|im_start\\|>assistant\\s*(.*)\", conversation, re.DOTALL)\n",
    "    if match:\n",
    "        assistant_answer = match.group(1).strip()\n",
    "    else:\n",
    "        assistant_answer = (\"i has no answer!\")\n",
    "    return assistant_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T20:29:36.89578Z",
     "iopub.status.busy": "2024-07-31T20:29:36.894921Z",
     "iopub.status.idle": "2024-07-31T20:29:36.901397Z",
     "shell.execute_reply": "2024-07-31T20:29:36.900326Z",
     "shell.execute_reply.started": "2024-07-31T20:29:36.895742Z"
    }
   },
   "outputs": [],
   "source": [
    "# вопросы к chandler\n",
    "q1 = \"c'mon, you're going out with the guy! there's gotta be something wrong with him!\"\n",
    "q2 = \"okay, everybody relax. this is not even a date. it's just two people going out to dinner and- not having sex.\"\n",
    "q3 = \"oh, yeah. had that dream.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тренированая модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"no, i'm not going out with him! i'm just going out with the guy! he's not the guy!\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(model, q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i know, i know, i'm sorry. but i'm really, really, really, really, really, really,\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(model, q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oh, yeah? what happened? what happened? what happened? what happened? what happened? what happened? what happened?'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(model, q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_anwer = []\n",
    "for i in dataset_sh['test']['q']:\n",
    "    learned_anwer.append(generate_answer(model, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98.9010989010989"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Загружаем пайплайн для классификации (предполагается, что модель fine-tuned сохранена в \"./results\")\n",
    "classifier = pipeline(\"text-classification\", model=model_classific, tokenizer=tokenizer_classification)\n",
    "\n",
    "\n",
    "\n",
    "# Классифицируем каждую реплику\n",
    "predictions = classifier(learned_anwer)\n",
    "\n",
    "# Предполагаем, что модель обучалась так, что \"LABEL_1\" соответствует репликам Chandler,\n",
    "# а \"LABEL_0\" — остальным.\n",
    "num_chandler = sum(1 for pred in predictions if pred['label'] == \"LABEL_1\")\n",
    "total = len(learned_anwer)\n",
    "percentage = (num_chandler / total) * 100\n",
    "percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Не тренированая модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T20:28:13.860754Z",
     "iopub.status.busy": "2024-07-31T20:28:13.859897Z",
     "iopub.status.idle": "2024-07-31T20:29:29.136779Z",
     "shell.execute_reply": "2024-07-31T20:29:29.135671Z",
     "shell.execute_reply.started": "2024-07-31T20:28:13.86072Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=cfg.torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "casual_model = AutoModelForCausalLM.from_pretrained(\n",
    "    cfg.model_name,\n",
    "    quantization_config=bnb_config,\n",
    "#     device_map=\"auto\",\n",
    "    attn_implementation=cfg.attn_implementation\n",
    ")\n",
    "\n",
    "tokenizer = tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
    "# tokenizer.padding_side = 'right'\n",
    "# tokenizer.padding_token = '<|pad_token|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T20:29:29.13895Z",
     "iopub.status.busy": "2024-07-31T20:29:29.138645Z",
     "iopub.status.idle": "2024-07-31T20:29:29.198595Z",
     "shell.execute_reply": "2024-07-31T20:29:29.197438Z",
     "shell.execute_reply.started": "2024-07-31T20:29:29.138923Z"
    }
   },
   "outputs": [],
   "source": [
    "if hasattr(tokenizer, \"chat_template\") and tokenizer.chat_template is not None:\n",
    "    tokenizer.chat_template = None  # Reset the chat template\n",
    "casual_model, tokenizer = setup_chat_format(casual_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T20:29:40.007186Z",
     "iopub.status.busy": "2024-07-31T20:29:40.006418Z",
     "iopub.status.idle": "2024-07-31T20:30:00.561813Z",
     "shell.execute_reply": "2024-07-31T20:30:00.560849Z",
     "shell.execute_reply.started": "2024-07-31T20:29:40.00715Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"c\\'mon, you\\'re going out with the guy! there\\'s gotta be something wrong with him!\"<|eot_id|><|start_header_id|>assistant'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(casual_model, q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"okay, everybody relax. this is not even a date. it\\'s just two people going out to dinner and- not'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(casual_model, q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oh, yeah. had that dream.<|eot_id|><|start_header_id|>assistant [PTM]<|eot_id|><|start_header_id|>assistant [PTM]<|eot_id|><|start_header_id|>assistant'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(casual_model, q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oh, yeah. had that dream.<|eot_id|><|start_header_id|>assistant [English]<|eot_id|><|start_header_id|>assistant [French]<|eot_id|><|start_header_id|>assistant [German'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(casual_model, q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlearned_anwer = []\n",
    "for i in dataset_sh['test']['q']:\n",
    "    unlearned_anwer.append(generate_answer(casual_model, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51.52625152625152"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Загружаем пайплайн для классификации\n",
    "classifier = pipeline(\"text-classification\", model=model_classific, tokenizer=tokenizer_classification)\n",
    "\n",
    "\n",
    "\n",
    "# Классифицируем каждую реплику\n",
    "predictions = classifier(unlearned_anwer)\n",
    "\n",
    "# Предполагаем, что модель обучалась так, что \"LABEL_1\" соответствует репликам Chandler,\n",
    "# а \"LABEL_0\" — остальным.\n",
    "num_chandler = sum(1 for pred in predictions if pred['label'] == \"LABEL_1\")\n",
    "total = len(unlearned_anwer)\n",
    "percentage = (num_chandler / total) * 100\n",
    "percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## выводы  \n",
    "\n",
    "визуально - наблюдается повышение качества генерации модели после дообучения.  \n",
    "классификатор - показывает рост качества ответов в 2 раза. а точнее 99% ответов модели можно считать репликами Чандлера.\n",
    "\n",
    "была выбрана слишком \"маленькая\" модель на 1 миллиард параметров. до обучения она плохо ведет диалог что можно видеть на примерах   \n",
    "после fine-tuning ситуация улучшилась, но все равно желательно взять модель не менее 7b\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Прото инференс на gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка сохраненнной модели\n",
    "\n",
    "path_to_save = \"Llama-finetuned\"\n",
    "\n",
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from trl import setup_chat_format\n",
    "\n",
    "saved_model =  AutoPeftModelForCausalLM.from_pretrained(path_to_save, torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(path_to_save)\n",
    "if hasattr(tokenizer, \"chat_template\") and tokenizer.chat_template is not None:\n",
    "    tokenizer.chat_template = None  # Reset the chat template\n",
    "saved_model, tokenizer = setup_chat_format(saved_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# инференс на gradio\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "saved_model.to('cuda')\n",
    "\n",
    "def chatbot(user_input, history):\n",
    "\n",
    "\n",
    "\n",
    "    if history is None:\n",
    "        history = []\n",
    "    # Формируем строку диалога из истории переписки\n",
    "    conversation = \"\"\n",
    "    for user_text, bot_text in history:\n",
    "        conversation += f\"User: {user_text}\\nBot: {bot_text}\\n\"\n",
    "    conversation += f\"User: {user_input}\\nBot: \"\n",
    "    \n",
    "    print(user_input)\n",
    "    \n",
    "    # Извлекаем ответ модели \n",
    "    bot_response = generate_answer(saved_model, user_input)\n",
    "    # Обновляем историю переписки\n",
    "    history.append((user_input, bot_response))\n",
    "    return history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot_ui = gr.Chatbot(label=\"Чат с моделью\")\n",
    "    state = gr.State([])\n",
    "    with gr.Row():\n",
    "        txt = gr.Textbox(show_label=False, placeholder=\"Введите сообщение и нажмите Enter\")\n",
    "    txt.submit(chatbot, [txt, state], chatbot_ui)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка к инференсу через LLAMA.CPP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединим слои LORA  в базовую модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_mu = saved_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_mu.save_pretrained('chandler')\n",
    "tokenizer.save_pretrained('chandler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('chandler\\\\tokenizer_config.json',\n",
       " 'chandler\\\\special_tokens_map.json',\n",
       " 'chandler\\\\tokenizer.json')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('chandler')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "скачаем репозиторий llama.cpp для конвертации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "конвертируем в gguf для инеференса с помощью LLAMA.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: chandler\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {32}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> Q8_0, shape = {2048, 128258}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> Q8_0, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> Q8_0, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> Q8_0, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> Q8_0, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 131072\n",
      "INFO:hf-to-gguf:gguf: embedding length = 2048\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 7\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128256\n",
      "INFO:gguf.vocab:Setting special token type eos to 128257\n",
      "INFO:gguf.vocab:Setting special token type pad to 128257\n",
      "INFO:gguf.vocab:Setting add_bos_token to True\n",
      "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:llama_chandler_1b_4q.gguf: n_tensors = 147, total_size = 1.3G\n",
      "\n",
      "Writing:   0%|          | 0.00/1.31G [00:00<?, ?byte/s]\n",
      "Writing:  21%|██▏       | 279M/1.31G [00:09<00:36, 28.3Mbyte/s]\n",
      "Writing:  23%|██▎       | 297M/1.31G [00:10<00:34, 29.7Mbyte/s]\n",
      "Writing:  24%|██▍       | 315M/1.31G [00:10<00:31, 31.3Mbyte/s]\n",
      "Writing:  25%|██▌       | 333M/1.31G [00:10<00:29, 33.2Mbyte/s]\n",
      "Writing:  26%|██▌       | 338M/1.31G [00:10<00:28, 33.9Mbyte/s]\n",
      "Writing:  28%|██▊       | 362M/1.31G [00:11<00:24, 38.8Mbyte/s]\n",
      "Writing:  29%|██▉       | 379M/1.31G [00:11<00:22, 41.5Mbyte/s]\n",
      "Writing:  30%|███       | 397M/1.31G [00:11<00:20, 44.1Mbyte/s]\n",
      "Writing:  31%|███       | 407M/1.31G [00:12<00:19, 46.1Mbyte/s]\n",
      "Writing:  32%|███▏      | 426M/1.31G [00:12<00:18, 49.0Mbyte/s]\n",
      "Writing:  34%|███▍      | 444M/1.31G [00:12<00:16, 51.5Mbyte/s]\n",
      "Writing:  35%|███▌      | 462M/1.31G [00:12<00:16, 52.8Mbyte/s]\n",
      "Writing:  36%|███▌      | 472M/1.31G [00:13<00:15, 54.6Mbyte/s]\n",
      "Writing:  37%|███▋      | 491M/1.31G [00:13<00:14, 58.3Mbyte/s]\n",
      "Writing:  39%|███▊      | 509M/1.31G [00:13<00:13, 58.1Mbyte/s]\n",
      "Writing:  40%|████      | 526M/1.31G [00:14<00:14, 55.1Mbyte/s]\n",
      "Writing:  41%|████      | 537M/1.31G [00:14<00:13, 56.7Mbyte/s]\n",
      "Writing:  42%|████▏     | 555M/1.31G [00:14<00:13, 58.0Mbyte/s]\n",
      "Writing:  44%|████▎     | 573M/1.31G [00:14<00:12, 57.7Mbyte/s]\n",
      "Writing:  45%|████▌     | 591M/1.31G [00:15<00:12, 57.8Mbyte/s]\n",
      "Writing:  46%|████▌     | 601M/1.31G [00:15<00:12, 57.7Mbyte/s]\n",
      "Writing:  47%|████▋     | 620M/1.31G [00:16<00:24, 28.7Mbyte/s]\n",
      "Writing:  49%|████▊     | 638M/1.31G [00:16<00:19, 34.1Mbyte/s]\n",
      "Writing:  50%|████▉     | 656M/1.31G [00:18<00:25, 25.7Mbyte/s]\n",
      "Writing:  50%|█████     | 661M/1.31G [00:18<00:23, 27.3Mbyte/s]\n",
      "Writing:  52%|█████▏    | 685M/1.31G [00:18<00:19, 33.0Mbyte/s]\n",
      "Writing:  53%|█████▎    | 703M/1.31G [00:19<00:23, 26.3Mbyte/s]\n",
      "Writing:  55%|█████▍    | 720M/1.31G [00:19<00:19, 30.5Mbyte/s]\n",
      "Writing:  55%|█████▌    | 726M/1.31G [00:20<00:18, 32.2Mbyte/s]\n",
      "Writing:  57%|█████▋    | 749M/1.31G [00:21<00:24, 23.4Mbyte/s]\n",
      "Writing:  58%|█████▊    | 767M/1.31G [00:23<00:34, 16.0Mbyte/s]\n",
      "Writing:  60%|█████▉    | 785M/1.31G [00:24<00:32, 16.3Mbyte/s]\n",
      "Writing:  61%|██████    | 795M/1.31G [00:25<00:34, 15.2Mbyte/s]\n",
      "Writing:  62%|██████▏   | 814M/1.31G [00:26<00:29, 16.7Mbyte/s]\n",
      "Writing:  63%|██████▎   | 832M/1.31G [00:27<00:27, 17.4Mbyte/s]\n",
      "Writing:  65%|██████▍   | 850M/1.31G [00:28<00:27, 17.1Mbyte/s]\n",
      "Writing:  65%|██████▌   | 855M/1.31G [00:28<00:24, 18.4Mbyte/s]\n",
      "Writing:  67%|██████▋   | 879M/1.31G [00:29<00:20, 21.0Mbyte/s]\n",
      "Writing:  68%|██████▊   | 896M/1.31G [00:29<00:17, 24.2Mbyte/s]\n",
      "Writing:  70%|██████▉   | 914M/1.31G [00:30<00:13, 28.6Mbyte/s]\n",
      "Writing:  70%|███████   | 920M/1.31G [00:30<00:13, 30.1Mbyte/s]\n",
      "Writing:  70%|███████   | 924M/1.31G [00:30<00:13, 29.9Mbyte/s]\n",
      "Writing:  72%|███████▏  | 943M/1.31G [00:30<00:11, 32.1Mbyte/s]\n",
      "Writing:  73%|███████▎  | 961M/1.31G [00:32<00:21, 16.3Mbyte/s]\n",
      "Writing:  75%|███████▍  | 979M/1.31G [00:33<00:17, 19.2Mbyte/s]\n",
      "Writing:  75%|███████▍  | 985M/1.31G [00:33<00:15, 21.0Mbyte/s]\n",
      "Writing:  77%|███████▋  | 1.01G/1.31G [00:34<00:10, 28.9Mbyte/s]\n",
      "Writing:  78%|███████▊  | 1.03G/1.31G [00:34<00:11, 24.9Mbyte/s]\n",
      "Writing:  79%|███████▉  | 1.04G/1.31G [00:35<00:11, 23.9Mbyte/s]\n",
      "Writing:  80%|████████  | 1.05G/1.31G [00:35<00:09, 27.1Mbyte/s]\n",
      "Writing:  82%|████████▏ | 1.07G/1.31G [00:37<00:10, 22.2Mbyte/s]\n",
      "Writing:  83%|████████▎ | 1.09G/1.31G [00:37<00:09, 23.7Mbyte/s]\n",
      "Writing:  84%|████████▍ | 1.11G/1.31G [00:38<00:07, 28.7Mbyte/s]\n",
      "Writing:  85%|████████▌ | 1.12G/1.31G [00:38<00:06, 32.2Mbyte/s]\n",
      "Writing:  87%|████████▋ | 1.14G/1.31G [00:38<00:05, 34.3Mbyte/s]\n",
      "Writing:  88%|████████▊ | 1.16G/1.31G [00:40<00:06, 22.7Mbyte/s]\n",
      "Writing:  89%|████████▉ | 1.17G/1.31G [00:40<00:05, 27.6Mbyte/s]\n",
      "Writing:  90%|████████▉ | 1.18G/1.31G [00:40<00:04, 28.9Mbyte/s]\n",
      "Writing:  90%|█████████ | 1.18G/1.31G [00:41<00:07, 17.8Mbyte/s]\n",
      "Writing:  92%|█████████▏| 1.20G/1.31G [00:42<00:05, 20.5Mbyte/s]\n",
      "Writing:  93%|█████████▎| 1.22G/1.31G [00:42<00:03, 25.8Mbyte/s]\n",
      "Writing:  94%|█████████▍| 1.24G/1.31G [00:43<00:02, 29.5Mbyte/s]\n",
      "Writing:  95%|█████████▍| 1.24G/1.31G [00:43<00:02, 31.3Mbyte/s]\n",
      "Writing:  96%|█████████▋| 1.27G/1.31G [00:43<00:01, 39.5Mbyte/s]\n",
      "Writing:  98%|█████████▊| 1.28G/1.31G [00:43<00:00, 42.9Mbyte/s]\n",
      "Writing:  99%|█████████▉| 1.30G/1.31G [00:44<00:00, 45.9Mbyte/s]\n",
      "Writing: 100%|█████████▉| 1.31G/1.31G [00:44<00:00, 48.2Mbyte/s]\n",
      "Writing: 100%|██████████| 1.31G/1.31G [00:44<00:00, 29.4Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to llama_chandler_1b_4q.gguf\n"
     ]
    }
   ],
   "source": [
    "!python ./llama.cpp/convert_hf_to_gguf.py ./chandler  --outfile llama_chandler_1b_4q.gguf   --outtype q8_0 "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
